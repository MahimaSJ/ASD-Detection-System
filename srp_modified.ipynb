{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDUB6tizEdjU",
        "outputId": "43bafcc5-3214-48fa-daa4-20d49fb589bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "fJW7IztuEpjy",
        "outputId": "3555571d-0fe6-428f-be57-747c92ee2794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe==0.10.5\n",
            "  Downloading mediapipe-0.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.5) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.5) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.5) (25.2.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.5) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.5) (2.2.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.5) (4.11.0.86)\n",
            "Collecting protobuf<4,>=3.11 (from mediapipe==0.10.5)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.5)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.5) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.5) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.5) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.5) (1.17.0)\n",
            "Downloading mediapipe-0.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.5 protobuf-3.20.3 sounddevice-0.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "465d45b444f94e5f8bf9a7e3a1765538",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install mediapipe==0.10.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQT82bbtRbZI",
        "outputId": "db8eea81-13c4-49d5-c033-e90bf524cd51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.5)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.2.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.14.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.1)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy>=1.24.4 (from albumentations)\n",
            "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf, numpy\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2 protobuf-3.20.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ad4b8eff2a894b5991a214adb1f1271f",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install albumentations opencv-python tensorflow mediapipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzEtSiuwEi4U",
        "outputId": "03e954dd-4ada-4300-a40c-1df08c96d3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jnV5zIgR-ndD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aWALKCEUZTp",
        "outputId": "f1fb2b14-e5a3-4261-f3d6-afcb85329613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed vid22.mp4, extracted 51 frames.\n",
            "Processed vid45.mp4, extracted 154 frames.\n",
            "Processed vid27.mp4, extracted 68 frames.\n",
            "Processed vid55.mp4, extracted 104 frames.\n",
            "Processed vid53.mp4, extracted 31 frames.\n",
            "Processed vid40.mp4, extracted 19 frames.\n",
            "Processed vid25.mp4, extracted 161 frames.\n",
            "Processed vid47.mp4, extracted 94 frames.\n",
            "Processed vid35.mp4, extracted 127 frames.\n",
            "Processed vid54.mp4, extracted 23 frames.\n",
            "Processed vid48.mp4, extracted 69 frames.\n",
            "Processed vid51.mp4, extracted 159 frames.\n",
            "Processed vid50.mp4, extracted 110 frames.\n",
            "Processed vid49.mp4, extracted 59 frames.\n",
            "Processed vid28.mp4, extracted 115 frames.\n",
            "Processed vid34.mp4, extracted 44 frames.\n",
            "Processed vid41.mp4, extracted 286 frames.\n",
            "Processed vid38.mp4, extracted 59 frames.\n",
            "Processed vid20.mp4, extracted 76 frames.\n",
            "Processed vid30.mp4, extracted 259 frames.\n",
            "Processed vid12.mp4, extracted 173 frames.\n",
            "Processed vid57.mp4, extracted 266 frames.\n",
            "Processed vid15.mp4, extracted 333 frames.\n",
            "Processed vid24.mp4, extracted 182 frames.\n",
            "Processed vid17.mp4, extracted 396 frames.\n",
            "Processed vid29.mp4, extracted 85 frames.\n",
            "Processed vid42.mp4, extracted 446 frames.\n",
            "Processed vid32.mp4, extracted 137 frames.\n",
            "Processed vid33.mp4, extracted 343 frames.\n",
            "Processed vid4.mp4, extracted 46 frames.\n",
            "Processed vid31.mp4, extracted 233 frames.\n",
            "Processed vid56.mp4, extracted 325 frames.\n",
            "Processed vid6.mp4, extracted 71 frames.\n",
            "Processed vid37.mp4, extracted 81 frames.\n",
            "Processed vid39.mp4, extracted 205 frames.\n",
            "Processed vid13.mp4, extracted 68 frames.\n",
            "Processed vid23.mp4, extracted 146 frames.\n",
            "Processed vid46.mp4, extracted 53 frames.\n",
            "Processed vid36.mp4, extracted 207 frames.\n",
            "Processed vid52.mp4, extracted 170 frames.\n",
            "Processed vid19.mp4, extracted 203 frames.\n",
            "Processed vid14.mp4, extracted 163 frames.\n",
            "Processed vid1.mp4, extracted 89 frames.\n",
            "Processed vid26.mp4, extracted 98 frames.\n",
            "Processed vid18.mp4, extracted 885 frames.\n",
            "Processed vid9.mp4, extracted 232 frames.\n",
            "Processed vid43.mp4, extracted 608 frames.\n",
            "Processed vid2.mp4, extracted 492 frames.\n",
            "Processed vid44.mp4, extracted 322 frames.\n",
            "Processed vid58.mp4, extracted 505 frames.\n",
            "Processed vid7.mp4, extracted 204 frames.\n",
            "Processed vid8.mp4, extracted 268 frames.\n",
            "Processed vid11.mp4, extracted 201 frames.\n",
            "Processed vid21.mp4, extracted 782 frames.\n",
            "Processed vid16.mp4, extracted 729 frames.\n",
            "Processed vid5.mp4, extracted 600 frames.\n",
            "Processed nvid20.mp4, extracted 29 frames.\n",
            "Processed nvid28.mp4, extracted 35 frames.\n",
            "Processed nvid24.mp4, extracted 31 frames.\n",
            "Processed nvid29.mp4, extracted 33 frames.\n",
            "Processed nvid39.mp4, extracted 17 frames.\n",
            "Processed nvid22.mp4, extracted 39 frames.\n",
            "Processed nvid25.mp4, extracted 44 frames.\n",
            "Processed nvid18.mp4, extracted 79 frames.\n",
            "Processed nvid34.mp4, extracted 14 frames.\n",
            "Processed nvid19.mp4, extracted 42 frames.\n",
            "Processed nvid37.mp4, extracted 22 frames.\n",
            "Processed nvid27.mp4, extracted 20 frames.\n",
            "Processed nvid41.mp4, extracted 14 frames.\n",
            "Processed nvid21.mp4, extracted 14 frames.\n",
            "Processed nvid47.mp4, extracted 21 frames.\n",
            "Processed nvid5.mp4, extracted 63 frames.\n",
            "Processed nvid48.mp4, extracted 36 frames.\n",
            "Processed nvid8.mp4, extracted 29 frames.\n",
            "Processed nvid36.mp4, extracted 19 frames.\n",
            "Processed nvid32.mp4, extracted 30 frames.\n",
            "Processed nvid13.mp4, extracted 580 frames.\n",
            "Processed nvid9.mp4, extracted 21 frames.\n",
            "Processed nvid6.mp4, extracted 27 frames.\n",
            "Processed nvid46.mp4, extracted 20 frames.\n",
            "Processed nvid23.mp4, extracted 38 frames.\n",
            "Processed nvid42.mp4, extracted 21 frames.\n",
            "Processed nvid33.mp4, extracted 15 frames.\n",
            "Processed nvid30.mp4, extracted 30 frames.\n",
            "Processed nvid16.mp4, extracted 79 frames.\n",
            "Processed nvid38.mp4, extracted 20 frames.\n",
            "Processed nvid31.mp4, extracted 18 frames.\n",
            "Processed nvid15.mp4, extracted 25 frames.\n",
            "Processed nvid2.mp4, extracted 72 frames.\n",
            "Processed nvid7.mp4, extracted 29 frames.\n",
            "Processed nvid44.mp4, extracted 26 frames.\n",
            "Processed nvid45.mp4, extracted 30 frames.\n",
            "Processed nvid40.mp4, extracted 26 frames.\n",
            "Processed nvid17.mp4, extracted 72 frames.\n",
            "Processed nvid35.mp4, extracted 25 frames.\n",
            "Processed v3.mp4, extracted 687 frames.\n",
            "Processed nvid14.mp4, extracted 66 frames.\n",
            "Processed nvid3.mp4, extracted 75 frames.\n",
            "Processed nvid4.mp4, extracted 78 frames.\n",
            "Processed nvid10.mp4, extracted 124 frames.\n",
            "Processed The Baby Human - Scale Error.mp4, extracted 368 frames.\n",
            "Processed nvid1.mp4, extracted 94 frames.\n",
            "Processed vid57.mp4, extracted 266 frames.\n",
            "Processed nvid1(1).mp4, extracted 403 frames.\n",
            "Processed video1.mp4, extracted 413 frames.\n",
            "Processed nvid43.mp4, extracted 21 frames.\n",
            "Processed nvid26.mp4, extracted 32 frames.\n",
            "Processed nvid49.mp4, extracted 1814 frames.\n",
            "Processed nvid11.mp4, extracted 1855 frames.\n",
            "Processed nvid12.mp4, extracted 703 frames.\n",
            "Frames saved in /content/drive/MyDrive/ASD/frames\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define paths\n",
        "base_path = \"/content/drive/MyDrive/ASD\"\n",
        "autistic_videos = os.path.join(base_path, \"autistic\")\n",
        "nonautistic_videos = os.path.join(base_path, \"nonautistic\")\n",
        "output_folder = os.path.join(base_path, \"frames\")\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "def extract_frames(video_folder, label, augment=False):\n",
        "    for video_name in os.listdir(video_folder):\n",
        "        video_path = os.path.join(video_folder, video_name)\n",
        "        video_id = os.path.splitext(video_name)[0]\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        frame_count, saved_frames = 0, 0\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % 10 == 0:\n",
        "                frame_filename = os.path.join(output_folder, f\"{video_id}_frame_{saved_frames}_{label}.jpg\")\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "                if augment:\n",
        "                    flipped_frame = cv2.flip(frame, 1)\n",
        "                    flipped_filename = os.path.join(output_folder, f\"{video_id}_flipped_{saved_frames}_{label}.jpg\")\n",
        "                    cv2.imwrite(flipped_filename, flipped_frame)\n",
        "\n",
        "                saved_frames += 1\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        print(f\"Processed {video_name}, extracted {saved_frames} frames.\")\n",
        "\n",
        "\n",
        "extract_frames(autistic_videos, \"autistic\")\n",
        "extract_frames(nonautistic_videos, \"nonautistic\", augment=True)\n",
        "\n",
        "print(f\"Frames saved in {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_WhsqRfmWdU4",
        "outputId": "b04f470a-6557-4759-b019-4a6f57921f35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pose and hand estimation applied to frames in /content/drive/MyDrive/ASD/pose_frames\n"
          ]
        }
      ],
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_hands = mp.solutions.hands\n",
        "pose = mp_pose.Pose()\n",
        "hands = mp_hands.Hands()\n",
        "\n",
        "pose_output_folder = os.path.join(base_path, \"pose_frames\")\n",
        "os.makedirs(pose_output_folder, exist_ok=True)\n",
        "\n",
        "for filename in sorted(os.listdir(output_folder)):\n",
        "    img_path = os.path.join(output_folder, filename)\n",
        "    image = cv2.imread(img_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    pose_results = pose.process(image_rgb)\n",
        "    hand_results = hands.process(image_rgb)\n",
        "\n",
        "    if pose_results.pose_landmarks:\n",
        "        for landmark in pose_results.pose_landmarks.landmark:\n",
        "            x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
        "            cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
        "\n",
        "    if hand_results.multi_hand_landmarks:\n",
        "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
        "            for landmark in hand_landmarks.landmark:\n",
        "                x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
        "                cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
        "\n",
        "    output_path = os.path.join(pose_output_folder, filename)\n",
        "    cv2.imwrite(output_path, image)\n",
        "\n",
        "print(f\"Pose and hand estimation applied to frames in {pose_output_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "img_size = (64, 64)\n",
        "sequence_length = 10\n",
        "pose_output_folder = \"/content/drive/MyDrive/ASD/pose_frames\"\n",
        "save_path = \"/content/drive/MyDrive/ASD/processed_sequences.npy\"\n",
        "labels_path = \"/content/drive/MyDrive/ASD/processed_labels.npy\"\n",
        "\n",
        "all_images = sorted(os.listdir(pose_output_folder))\n",
        "if len(all_images) < sequence_length:\n",
        "    raise ValueError(f\"Not enough images in {pose_output_folder} to form a sequence.\")\n",
        "\n",
        "def load_image(img_path):\n",
        "    \"\"\"Reads and resizes an image, returns None if unreadable.\"\"\"\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "    if img is None:\n",
        "        return None\n",
        "    return cv2.resize(img, img_size) / 255.0\n",
        "\n",
        "X_train, y_train = [], []\n",
        "batch_size = 500\n",
        "\n",
        "print(\"Processing sequences efficiently...\")\n",
        "\n",
        "for i in range(len(all_images) - sequence_length):\n",
        "    img_paths = [os.path.join(pose_output_folder, all_images[i + j]) for j in range(sequence_length)]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        frames = list(executor.map(load_image, img_paths))\n",
        "\n",
        "    if any(frame is None for frame in frames):\n",
        "        continue\n",
        "\n",
        "    label = 1 if \"autistic\" in all_images[i].lower() else 0\n",
        "    X_train.append(frames)\n",
        "    y_train.append(label)\n",
        "\n",
        "    if len(X_train) >= batch_size:\n",
        "        np.save(save_path, np.array(X_train))\n",
        "        np.save(labels_path, np.array(y_train))\n",
        "        X_train, y_train = [],[]\n",
        "\n",
        "if X_train:\n",
        "    np.save(save_path, np.array(X_train))\n",
        "    np.save(labels_path, np.array(y_train))\n",
        "\n",
        "print(f\"Processing complete! Data saved at {save_path}\")\n"
      ],
      "metadata": {
        "id": "4vNSXVGr96e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728c58ce-aa94-40e3-8d19-ba763dac0c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sequences efficiently...\n",
            "Processing complete! Data saved at /content/drive/MyDrive/ASD/processed_sequences.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCgENKlArmI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/ASD/processed_sequences.npy\"\n",
        "labels_path = \"/content/drive/MyDrive/ASD/processed_labels.npy\"\n",
        "\n",
        "print(\"Loading processed data...\")\n",
        "X_train = np.load(save_path)\n",
        "y_train = np.load(labels_path)\n",
        "\n",
        "print(f\"Data Shape: {X_train.shape}, Labels Shape: {y_train.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], -1)\n",
        "\n",
        "print(f\"Reshaped X_train shape: {X_train_reshaped.shape}\")\n",
        "print(f\"Reshaped X_val shape: {X_val_reshaped.shape}\")\n",
        "\n",
        "print(\"Building the BiLSTM model...\")\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),  # Adjusted Input Shape\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(X_train_reshaped, y_train, validation_data=(X_val_reshaped, y_val), epochs=20, batch_size=32)\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/ASD/asd_bilstm_model.h5\"\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model training complete! Saved at {model_save_path}\")\n",
        "\n",
        "saved_model_path = \"/content/drive/MyDrive/ASD/asd_bilstm_model\"\n",
        "tf.saved_model.save(model, saved_model_path)\n",
        "\n",
        "print(f\"Model saved for deployment at {saved_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3yuh2PrDrmWZ",
        "outputId": "a723508d-f3ad-43ab-b26a-5085a20c825a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processed data...\n",
            "Data Shape: (13, 10, 64, 64, 3), Labels Shape: (13,)\n",
            "Reshaped X_train shape: (10, 10, 12288)\n",
            "Reshaped X_val shape: (3, 10, 12288)\n",
            "Building the BiLSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m6,324,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m41,216\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,040\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,324,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,367,009\u001b[0m (24.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,367,009</span> (24.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,367,009\u001b[0m (24.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,367,009</span> (24.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step - accuracy: 0.2000 - loss: 0.7446 - val_accuracy: 1.0000 - val_loss: 0.3688\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.9000 - loss: 0.4396 - val_accuracy: 1.0000 - val_loss: 0.2327\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step - accuracy: 1.0000 - loss: 0.2716 - val_accuracy: 1.0000 - val_loss: 0.1705\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504ms/step - accuracy: 1.0000 - loss: 0.2326 - val_accuracy: 1.0000 - val_loss: 0.1245\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 1.0000 - loss: 0.1870 - val_accuracy: 1.0000 - val_loss: 0.0965\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step - accuracy: 1.0000 - loss: 0.1787 - val_accuracy: 1.0000 - val_loss: 0.0749\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step - accuracy: 1.0000 - loss: 0.0954 - val_accuracy: 1.0000 - val_loss: 0.0583\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 0.0916 - val_accuracy: 1.0000 - val_loss: 0.0462\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 1.0000 - loss: 0.0415 - val_accuracy: 1.0000 - val_loss: 0.0374\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step - accuracy: 1.0000 - loss: 0.0553 - val_accuracy: 1.0000 - val_loss: 0.0306\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step - accuracy: 1.0000 - loss: 0.0385 - val_accuracy: 1.0000 - val_loss: 0.0254\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 1.0000 - loss: 0.0308 - val_accuracy: 1.0000 - val_loss: 0.0213\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 541ms/step - accuracy: 1.0000 - loss: 0.0249 - val_accuracy: 1.0000 - val_loss: 0.0180\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667ms/step - accuracy: 1.0000 - loss: 0.0305 - val_accuracy: 1.0000 - val_loss: 0.0153\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588ms/step - accuracy: 1.0000 - loss: 0.0297 - val_accuracy: 1.0000 - val_loss: 0.0131\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 1.0000 - loss: 0.0258 - val_accuracy: 1.0000 - val_loss: 0.0113\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 0.0086\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step - accuracy: 1.0000 - loss: 0.0237 - val_accuracy: 1.0000 - val_loss: 0.0076\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step - accuracy: 1.0000 - loss: 0.0204 - val_accuracy: 1.0000 - val_loss: 0.0067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete! Saved at /content/drive/MyDrive/ASD/asd_bilstm_model.h5\n",
            "Model saved for deployment at /content/drive/MyDrive/ASD/asd_bilstm_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnJbkt6frmnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Model Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Predictions\n",
        "y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "hGFvc_MNpH2-",
        "outputId": "b67081eb-24e6-4059-ba66-368100605dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n",
            "Confusion Matrix:\n",
            " [[3]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/ASD/asd_bilstm_model\"\n",
        "tf.saved_model.save(model, model_save_path)\n",
        "\n",
        "print(f\"Model saved for deployment at {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIgeX4TApXY9",
        "outputId": "0f2ff6aa-aa29-4290-d5e5-7bb1a7467804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved for deployment at /content/drive/MyDrive/ASD/asd_bilstm_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(sequence_length, 12288)),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "-Ehn9RbWqAT-",
        "outputId": "1a53c228-2042-45bc-cc55-209bce234792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m6,324,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m41,216\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,040\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,324,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,367,009\u001b[0m (24.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,367,009</span> (24.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,367,009\u001b[0m (24.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,367,009</span> (24.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {0: 10., 1: 1.}\n",
        "history = model.fit(X_train_reshaped, y_train, validation_data=(X_val_reshaped, y_val), epochs=20, batch_size=32, class_weight=class_weights)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tMRm9yFqD2u",
        "outputId": "df5e6a59-1fd8-4ed3-c4b3-3b75a8747554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0060\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step - accuracy: 1.0000 - loss: 0.0324 - val_accuracy: 1.0000 - val_loss: 0.0054\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step - accuracy: 1.0000 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 0.0048\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614ms/step - accuracy: 1.0000 - loss: 0.0166 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451ms/step - accuracy: 1.0000 - loss: 0.0736 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step - accuracy: 1.0000 - loss: 0.0214 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step - accuracy: 1.0000 - loss: 0.0177 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656ms/step - accuracy: 1.0000 - loss: 0.0469 - val_accuracy: 1.0000 - val_loss: 0.0027\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560ms/step - accuracy: 1.0000 - loss: 0.0323 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step - accuracy: 1.0000 - loss: 0.0173 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 1.0000 - loss: 0.0629 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step - accuracy: 1.0000 - loss: 0.0148 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step - accuracy: 1.0000 - loss: 0.0112 - val_accuracy: 1.0000 - val_loss: 0.0013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_save_path = \"/content/drive/MyDrive/ASD/asd_bilstm_model.h5\"\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model training complete! Saved at {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfgqAt4Ypccb",
        "outputId": "8d16b01e-0269-4201-f442-e300b6c4335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete! Saved at /content/drive/MyDrive/ASD/asd_bilstm_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/ASD/asd_bilstm_model\"\n",
        "tf.saved_model.save(model, model_save_path)\n",
        "\n",
        "print(f\"Model saved for deployment at {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT93GOiZpiN5",
        "outputId": "3be3d336-0701-400a-ee2b-f9c97cd246e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved for deployment at /content/drive/MyDrive/ASD/asd_bilstm_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XgTPX0pd5aZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_frames(video_path, img_size=(64, 64), frame_skip=5):\n",
        "    \"\"\"\n",
        "    Extracts frames from a video at intervals and resizes them.\n",
        "    - frame_skip: Extracts every `frame_skip` frame to reduce redundancy.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "\n",
        "        if frame_count % frame_skip == 0:\n",
        "            frame = cv2.resize(frame, img_size) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames)\n"
      ],
      "metadata": {
        "id": "uLLP2IVuqiew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video_sequences(frames, sequence_length=10):\n",
        "    \"\"\"\n",
        "    Converts extracted frames into sequences for the model.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    for i in range(len(frames) - sequence_length + 1):\n",
        "        sequences.append(frames[i : i + sequence_length])\n",
        "\n",
        "    return np.array(sequences)"
      ],
      "metadata": {
        "id": "FkH5eQ1GqjxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_video(video_path, model, sequence_length=10):\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    if len(frames) < sequence_length:\n",
        "        print(\"Error: Video too short for prediction.\")\n",
        "        return\n",
        "\n",
        "    sequences = create_video_sequences(frames)\n",
        "\n",
        "    sequences_reshaped = sequences.reshape(sequences.shape[0], sequences.shape[1], -1)\n",
        "\n",
        "    predictions = model.predict(sequences_reshaped)\n",
        "\n",
        "    # Convert predictions to labels\n",
        "    avg_prediction = np.mean(predictions)\n",
        "    result = \"Autistic\" if avg_prediction >= 0.5 else \"Non-Autistic\"\n",
        "\n",
        "    print(f\"Prediction: {result} (Confidence: {avg_prediction:.2f})\")\n",
        "    return result, avg_prediction\n"
      ],
      "metadata": {
        "id": "4lP5ZQAgqmpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/ASD/test_video.mp4\"\n",
        "predict_video(video_path, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lvREiBBqpRD",
        "outputId": "b19f546a-89eb-4348-cdb6-721c08d6020d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "Prediction: Autistic (Confidence: 1.00)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Autistic', np.float32(0.9986423))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/ASD/test_video2.mp4\"\n",
        "predict_video(video_path, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnrPZ49hXcVs",
        "outputId": "2a8dbd46-a873-4689-cf74-9442c79b60f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Predicted Class: 0\n",
            "Predicted Class Label: Non-Autistic\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0000000e+00, 2.6234566e-30]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbbX0i-ONgAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqhEbxXvP20H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}